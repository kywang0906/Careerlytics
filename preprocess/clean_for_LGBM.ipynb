{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPydCvDNjI/EmaLOgAtsOob"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","FOLDERNAME = \"Colab\\ Notebooks/fetch-data\"\n","%cd drive/MyDrive/$FOLDERNAME"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2tOxR1nASPuK","executionInfo":{"status":"ok","timestamp":1750437955621,"user_tz":-480,"elapsed":3534,"user":{"displayName":"王廣瑜","userId":"13649321363543885870"}},"outputId":"761d55d9-8d8e-4749-d6b5-8af4d916dfb4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# !pip install Unidecode"],"metadata":{"id":"Jto_dGSRxb9q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","import json, ast\n","from unidecode import unidecode\n","from sklearn.preprocessing import OneHotEncoder"],"metadata":{"id":"LpuLFwZd1uPY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["FILENAME = 'merged_apple_swe.csv'\n","SPLIT_FILE_NAME = FILENAME.split('_')\n","COMPANY = SPLIT_FILE_NAME[1].capitalize()\n","ROLE = SPLIT_FILE_NAME[2].split('.')[0]\n","PATH = f'raw/{COMPANY}/'\n","OUTPUT = f'cleaned/LGBM/cleaned_{COMPANY.lower()}_{ROLE}.csv'"],"metadata":{"id":"L-O3JSJJmSsr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["LABELS = {\n","    'da': 0,\n","    'ds': 1,\n","    'pm': 2,\n","    'swe': 3\n","}"],"metadata":{"id":"WsbWITeTmb72"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Raw Dataframe"],"metadata":{"id":"SuTnhZibAmQp"}},{"cell_type":"code","source":["def parse_json_list(x):\n","    if pd.isna(x) or not isinstance(x, str) or x.strip() in ['', '[]']:\n","        return []\n","    s = x.strip()\n","    try:\n","        return json.loads(s)\n","    except json.JSONDecodeError:\n","        try:\n","            return ast.literal_eval(s)\n","        except (ValueError, SyntaxError):\n","            return []\n","\n","df = pd.read_csv(\n","    (PATH+FILENAME),\n","    converters={\n","        'education'     : parse_json_list,\n","        'experience'    : parse_json_list,\n","        'certifications': parse_json_list,\n","        'projects'      : parse_json_list,\n","        'publications'  : parse_json_list,\n","        'courses'       : parse_json_list\n","    }\n",")\n","df = df.drop_duplicates(subset='id')"],"metadata":{"id":"QS9dBYVnPp7r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Education (edu)"],"metadata":{"id":"92NkH6McBr6I"}},{"cell_type":"markdown","source":["#### Degree"],"metadata":{"id":"ajv18lZNH1lN"}},{"cell_type":"code","source":["def classify_degree(deg):\n","    \"\"\"\n","    Return (degree_cat, new_cert, new_course)：\n","      - degree_cat: \"Bachelor's\"/\"Master's\"/\"PhD\"/\"Associate\"/\"Other\"/\"Extracurricular\" or None\n","      - new_cert: Return deg or None based on keywords matching\n","      - new_course: Return deg or None based on keywords matching\n","    \"\"\"\n","    if not isinstance(deg, str) or not deg.strip():\n","        return (None, None, None)\n","    s = deg.strip()\n","\n","    # Official Degree\n","    deg_patterns = {\n","        \"Bachelor's\": re.compile(\n","            r\"\\bbachelor|学士|學士|licenciatura|laurea|licence\"\n","            r\"|(?<!m)BA\\b|(?<!m)B\\.A?\\b|(?<!m)BS\\b|(?<!m)B\\.S\\.?\\b|B\\.F\\.A\\.?|BFA\\b|BTech\\b|B\\.Tech?\"\n","            r\"|BE\\b|BEng\\b|B\\.Eng\\b|BSc\\b|B\\.Sc\\b|B\\.Com?|A\\.B?|\",\n","            re.IGNORECASE\n","        ),\n","        \"Master's\":   re.compile(\n","            r\"\\bmaster|硕士|碩士|máster|magistrale\"\n","            r\"|MS\\b|M\\.S\\.?\\b|MA\\b|M\\.A\\.?\\b\"\n","            r\"|MBA\\b|M\\.B\\.A\\.?\\b|MBS\\b|M\\.B\\.S\\.?|M\\.Tech?|MTech\"\n","            r\"|MSc\\b|M\\.Sc\\b|MPhil|M\\.Phil\\b|M\\.F\\.A\\.?|MFA\\b\",\n","            re.IGNORECASE\n","        ),\n","        \"PhD\":        re.compile(\n","            r\"\\bphd\\b|博士|doctorad|dottorato|doctorat|doktor|doctor\"\n","            r\"|PhD|Ph\\.?D\\.?\\b|DFA\\b|D\\.F\\.A\\b|J\\.D\\.?\\b|Dphil\\b|D\\.Phil\\.?|MD|DDS|M\\.D\\.?|D\\.D\\.S\\b\",\n","            re.IGNORECASE\n","        ),\n","        \"Associate\":  re.compile(\n","            r\"\\bassociate|副学士|副學士|專科|二技|五專|t[eé]cnico superior\"\n","            r\"|AS\\b|A\\.S\\.?\\b|AA\\b|A\\.A\\.?\\b|BTS\\b|DUT\\b|Fachhochschul\",\n","            re.IGNORECASE\n","        ),\n","    }\n","\n","    for cat, pat in deg_patterns.items():\n","        if pat.search(s):\n","            return (cat, None, None)\n","\n","    # Certificates\n","    cert_pattern = re.compile(r\"certificat|diploma|nanodegree\", re.IGNORECASE)\n","    if cert_pattern.search(s):\n","        return (None, s, None)\n","\n","    # Short-term studies\n","    short_course_re = re.compile(\n","        r\"bootcamp|workshop|program(me)?|summer|exchange|semester|abroad\", re.IGNORECASE\n","    )\n","    if short_course_re.search(s):\n","        return (\"Extracurricular\", None, None)\n","\n","    # Courses\n","    long_course_kw = re.compile(\n","        r\"Studies|Science|Engineering|Economics|Mathematics|Media|Computational|Physics|\"\n","        r\"Cyber|Management|Electronic|Préparatoire\",\n","        re.IGNORECASE\n","    )\n","    if long_course_kw.search(s):\n","        return (None, None, s)\n","\n","    # Other\n","    return (\"Other\", None, None)"],"metadata":{"id":"XzUXRc2UCMGR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate \"edu\"\n","edu = (\n","    df[['id','education']]\n","      .explode('education')\n","      .dropna(subset=['education'])\n","      .reset_index(drop=True)\n",")\n","edu = pd.DataFrame({\n","    'id':         edu['id'],\n","    'school':     edu['education'].map(lambda x: x.get('title') if isinstance(x, dict) else None),\n","    'degree_raw': edu['education'].map(lambda x: x.get('degree') if isinstance(x, dict) else None),\n","    'field':      edu['education'].map(lambda x: x.get('field') if isinstance(x, dict) else None),\n","})\n","\n","# apply classify_degree, expand=True\n","edu[['degree_cat','new_cert','new_course']] = (\n","    edu['degree_raw']\n","       .apply(classify_degree)\n","       .apply(pd.Series)\n",")\n","\n","degree_code = {\n","    \"PhD\": 0, \"Master's\": 1, \"Bachelor's\": 2,\n","    \"Associate\": 3, \"Other\": 5, \"Extracurricular\": 4\n","}\n","edu['degree_code'] = edu['degree_cat'].map(degree_code).fillna(6).astype(int)\n","\n","cert_orig = (\n","    df[['id','certifications']].explode('certifications')\n","      .dropna(subset=['certifications'])\n","      .pipe(lambda d: pd.DataFrame({\n","          'id': d['id'],\n","          'cert_title': d['certifications'].map(lambda x: x.get('title') if isinstance(x, dict) else x)\n","      }))\n",")\n","course_orig = (\n","    df[['id','courses']].explode('courses')\n","      .dropna(subset=['courses'])\n","      .pipe(lambda d: pd.DataFrame({\n","          'id': d['id'],\n","          'course_title': d['courses'].map(lambda x: x.get('title') if isinstance(x, dict) else x)\n","      }))\n",")\n","\n","# Get certifications / courses from edu\n","cert_new   = edu[['id','new_cert']].dropna(subset=['new_cert']).rename(columns={'new_cert':'cert_title'})\n","course_new = edu[['id','new_course']].dropna(subset=['new_course']).rename(columns={'new_course':'course_title'})\n","\n","# Concatinate & drop duplicates\n","cert_all   = pd.concat([cert_orig,   cert_new],   ignore_index=True).drop_duplicates()\n","course_all = pd.concat([course_orig, course_new], ignore_index=True).drop_duplicates()\n","\n","# Aggregate the data & merge back to df\n","cert_map   = cert_all.groupby('id')['cert_title'].agg(list)\n","course_map = course_all.groupby('id')['course_title'].agg(list)\n","\n","df['certifications'] = df['id'].map(lambda i: cert_map.get(i, []))\n","df['courses']        = df['id'].map(lambda i: course_map.get(i, []))"],"metadata":{"id":"M1QHw_7eHNFb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Field"],"metadata":{"id":"i5exN-ToJIWV"}},{"cell_type":"code","source":["# field → numbers\n","def map_field(f: str) -> int:\n","    if not isinstance(f, str) or not f.strip():\n","        return 5\n","    s = f.strip().lower()\n","    # (1) CS-Related\n","    if \"computer\" in s or \"software engineering\" in s or \"information engineering\" in s or \"artificial intelligence\" in s:\n","        return 0\n","    # (2) EE/ECE\n","    ee_patterns = [\n","        \"electrical and electronics engineering\",\n","        \"electrical engineering\",\n","        \"electronics and communications engineering\",\n","        \"electronics engineering\",\n","        r\"\\belectrical\\b\",\n","        r\"\\bee\\b\",\n","        r\"\\bece\\b\"\n","    ]\n","    if any(re.search(pat, s) for pat in ee_patterns):\n","        return 1\n","    # (3) Information / Data / Stats / Cyber / Math / Physics\n","    kw3 = [\"information\",\"informatics\",\"data\",\"statistic\",\"cyber\",\"mathematics\",\"physics\"]\n","    if any(kw in s for kw in kw3):\n","        return 2\n","    # (4) Business / Economics / Finance / Management / Marketing / Accounting\n","    kw4 = [\"business\",\"economics\",\"finance\",\"management\",\"marketing\",\"accounting\"]\n","    if any(kw in s for kw in kw4):\n","        return 3\n","    # (5) Other STEM\n","    if \"engineering\" in s or \"science\" in s:\n","        return 4\n","    # (6) Other\n","    return 5\n","\n","edu['field_code'] = edu['field'].apply(map_field)"],"metadata":{"id":"zTsLpQ0TD4Q-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### School Tier"],"metadata":{"id":"s6SMItZGH7Pd"}},{"cell_type":"code","source":["# Load THE CS Top100 List\n","rank_df = pd.read_csv('the_cs_top100.csv')  # 包含 rank, university, country\n","\n","# Normalize school names\n","def normalize(name: str) -> str:\n","    if not isinstance(name, str):\n","        return \"\"\n","    # Transfer non-Latin alphabets languages into latin alphabets\n","    s = unidecode(name)\n","    # Keep words only before '-'\n","    s = re.split(r'\\s*[-–—]\\s*', s, maxsplit=1)[0]\n","    # Remove common university-related words\n","    s = s.lower()\n","    s = re.sub(r'\\b(university|institute|college|da xue|school|of|and)\\b', '', s)\n","    # Keep spaces & alphabets only\n","    s = re.sub(r'[^a-z0-9 ]+', ' ', s)\n","    # Concatenate spaces\n","    s = re.sub(r'\\s+', ' ', s).strip()\n","    return s\n","\n","# Normalize school names in both school list & edu\n","rank_df['school_norm'] = rank_df['university'].map(normalize)\n","edu['school_norm']     = edu['school'].map(normalize)\n","\n","# Merge rankings to edu\n","edu = edu.merge(\n","    rank_df[['school_norm','rank']],\n","    on='school_norm',\n","    how='left'\n",")\n","\n","# Define Tier\n","def rank_to_tier(r):\n","    if pd.isna(r):        return 'Other'\n","    r = int(r)\n","    if r <= 20:           return 'Top20'\n","    if r <= 50:           return 'Top50'\n","    if r <= 100:          return 'Top100'\n","    return 'Other'\n","\n","edu['school_tier'] = edu['rank'].apply(rank_to_tier)\n","\n","tier_map = {\n","    'Top20':   0,\n","    'Top50':   1,\n","    'Top100':  2,\n","    'Other':   3\n","}\n","\n","# Create tier_code\n","edu['tier_code'] = edu['school_tier'].map(tier_map).fillna(3).astype(int)"],"metadata":{"id":"5H-XG3jiE5qo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["edu = edu[[\n","    'id','degree_code','field_code','tier_code'\n","]]"],"metadata":{"id":"RWZpvtSPrgaY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Experience (exp)"],"metadata":{"id":"Vz0ionSzCY2q"}},{"cell_type":"code","source":["import pandas as pd\n","import re\n","\n","# Assume COMPANY is defined (one of 'amazon','apple','google','meta','microsoft')\n","COMPANY = COMPANY.lower()\n","\n","# Normalize current company names to lowercase and strip whitespace\n","df['current_company_name'] = (\n","    df['current_company_name']\n","      .fillna('')\n","      .str.lower()\n","      .str.strip()\n",")\n","\n","# Define synonyms for each FAAMG company\n","synonyms = {\n","    'amazon':  ['amazon', 'aws'],\n","    'apple':   ['apple'],\n","    'google':  ['google', 'alphabet'],\n","    'meta':    ['meta', 'facebook'],\n","    'microsoft': ['microsoft']\n","}\n","\n","# Filter master DataFrame to only rows whose current company matches COMPANY synonyms\n","keys = synonyms.get(COMPANY, [COMPANY])\n","df = df[df['current_company_name']\n","        .apply(lambda name: any(k in name for k in keys))\n","     ].reset_index(drop=True)\n","\n","# Explode the 'experience' list into one row per entry\n","exp = (\n","    df[['id', 'current_company_name', 'experience']]\n","     .explode('experience')\n","     .reset_index(drop=True)\n",")\n","\n","# Map any company name containing FAANG keywords into a canonical FAANG label\n","def map_faang(name: str) -> str:\n","    s = name.lower()\n","    if 'amazon' in s or 'aws' in s:\n","        return 'amazon'\n","    if 'meta' in s or 'facebook' in s:\n","        return 'meta'\n","    if 'google' in s or 'alphabet' in s:\n","        return 'google'\n","    if 'microsoft' in s or 'msft' in s:\n","        return 'microsoft'\n","    if 'apple' in s:\n","        return 'apple'\n","    return None\n","\n","exp['faang_comp'] = exp['current_company_name'].apply(map_faang)\n","\n","# Keep only rows that were mapped to a FAANG company\n","exp = exp[exp['faang_comp'].notna()].copy()\n","\n","# Use that FAANG label as our normalized company column\n","exp['comp_norm'] = exp['faang_comp']\n","\n","# Extract subfields from the experience dict\n","exp['exp_company']     = exp['experience'].apply(lambda x: x.get('company')     if isinstance(x, dict) else None)\n","exp['exp_title']       = exp['experience'].apply(lambda x: x.get('title')       if isinstance(x, dict) else None)\n","exp['exp_description'] = exp['experience'].apply(lambda x: x.get('description') if isinstance(x, dict) else None)\n","exp['exp_duration']    = exp['experience'].apply(lambda x: x.get('duration')    if isinstance(x, dict) else None)\n","exp['exp_positions']   = exp['experience'].apply(\n","    lambda x: [p.get('subtitle') for p in x.get('positions', [])] if isinstance(x, dict) else []\n",")\n","\n","# Flag which rows are the “current company” vs. past experiences\n","exp['is_current_company'] = exp['comp_norm'] == exp['current_company_name']\n","exp['order']              = exp.groupby('id').cumcount()\n","first_current = exp[exp['is_current_company']].groupby('id')['order'].min()\n","exp['is_past_exp'] = exp.apply(\n","    lambda r: r['order'] < first_current.get(r['id'], float('inf')),\n","    axis=1\n",")\n","\n","# Mark past experiences at Fortune 500 companies\n","fortune500 = pd.read_csv('fortune500.csv')\n","f500_set   = set(fortune500['Company'].str.lower().str.strip())\n","exp['is_f500'] = exp.apply(\n","    lambda r: 1 if (r['is_past_exp'] and r['comp_norm'] in f500_set) else 0,\n","    axis=1\n",")\n","\n","# Parse “X years Y months” strings into total years (float)\n","def parse_duration(s: str) -> float:\n","    if not isinstance(s, str):\n","        return 0.0\n","    y_match = re.search(r'(\\d+)\\s*year', s)\n","    m_match = re.search(r'(\\d+)\\s*month', s)\n","    years   = int(y_match.group(1)) if y_match else 0\n","    months  = int(m_match.group(1)) if m_match else 0\n","    return years + months / 12\n","\n","exp['duration_years'] = exp['exp_duration'].apply(parse_duration)\n","\n","# Bucketize duration into categorical codes\n","def bucket_duration(years: float) -> int:\n","    if years > 10:\n","        return 0\n","    if years > 5:\n","        return 1\n","    if years > 2:\n","        return 2\n","    return 3\n","\n","exp['duration_bucket'] = exp['duration_years'].apply(bucket_duration)\n","\n","# Select the final columns you want to keep\n","exp = exp[[\n","    'id', 'exp_company', 'exp_title', 'exp_description',\n","    'exp_duration', 'duration_years', 'duration_bucket', 'exp_positions',\n","    'is_current_company', 'is_past_exp', 'is_f500', 'comp_norm'\n","]]\n","\n","# One-hot encode the normalized FAANG company labels—and force all five columns\n","ALL_COMP = ['amazon','apple','google','meta','microsoft']\n","ALL_COLS = [f'comp_{c}' for c in ALL_COMP]\n","\n","# generate dummies from comp_norm\n","comp_ohe = pd.get_dummies(exp['comp_norm'], prefix='comp')\n","\n","# reindex to ensure all five comp_* columns exist (missing ones filled with 0)\n","comp_ohe = comp_ohe.reindex(columns=ALL_COLS, fill_value=0)\n","\n","# concat back and cast to int\n","exp = pd.concat([exp, comp_ohe], axis=1)\n","exp[ALL_COLS] = exp[ALL_COLS].astype(int)"],"metadata":{"id":"bRFAHTuk2laW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Certifications (cert)"],"metadata":{"id":"zg2GhdNlIYub"}},{"cell_type":"code","source":["cert = (\n","    df[['id','certifications']]\n","      .explode('certifications')\n","      .dropna(subset=['certifications'])\n",")\n","cert = pd.DataFrame({\n","    'id': cert['id'],\n","    'cert_title': cert['certifications'].map(lambda x: x.get('title') if isinstance(x, dict) else x)\n","})"],"metadata":{"id":"ln2DvBk88WIf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Projects (proj)"],"metadata":{"id":"1VOZnwrsIb7H"}},{"cell_type":"code","source":["proj = (\n","    df[['id','projects']]\n","      .explode('projects')\n","      .dropna(subset=['projects'])\n",")\n","proj = pd.DataFrame({\n","    'id': proj['id'],\n","    'proj_title':       proj['projects'].map(lambda x: x.get('title')),\n","    'proj_description': proj['projects'].map(lambda x: x.get('description')),\n","})"],"metadata":{"id":"ha3XRrQzIdyL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Publications (pub)"],"metadata":{"id":"R2KV6fHkImdc"}},{"cell_type":"code","source":["pub = (\n","    df[['id','publications']]\n","      .explode('publications')\n","      .dropna(subset=['publications'])\n",")\n","pub = pd.DataFrame({\n","    'id':      pub['id'],\n","    'pub_title': pub['publications'].map(lambda x: x.get('title')),\n","})"],"metadata":{"id":"ZZ6FJTC_IoMD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Courses (course)"],"metadata":{"id":"1ySi8JybIszE"}},{"cell_type":"code","source":["course = (\n","    df[['id','courses']]\n","      .explode('courses')\n","      .dropna(subset=['courses'])\n",")\n","course = pd.DataFrame({\n","    'id':         course['id'],\n","    'course_title': course['courses'].map(lambda x: x.get('title') if isinstance(x, dict) else x)\n","})"],"metadata":{"id":"gmax7WnXIuMa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Merge tables"],"metadata":{"id":"LsAQURTYLWRg"}},{"cell_type":"code","source":["id_df = pd.DataFrame({'id': df['id'].unique()})"],"metadata":{"id":"VHsaKeR9NLbZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### edu"],"metadata":{"id":"PawMsrFlLlTk"}},{"cell_type":"code","source":["# Get best education info\n","best_idx = edu.groupby('id')['degree_code'].idxmin()\n","best_edu = (\n","    edu.loc[best_idx, ['id','degree_code','tier_code']]\n","       .drop_duplicates(subset=['id'])\n",")\n","\n","# Calculate field mode\n","field_mode = (\n","    edu.groupby('id')['field_code']\n","       .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n","       .reset_index()\n","       .rename(columns={'field_code':'field_mode'})\n",")\n","\n","# Left join Merge best_edu & field_mode to id_df\n","edu_agg = (\n","    id_df\n","      .merge(best_edu,    on='id', how='left')\n","      .merge(field_mode,  on='id', how='left')\n",")"],"metadata":{"id":"Mv9Ag6W9N7un"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### exp"],"metadata":{"id":"Ix4EGC25aQVO"}},{"cell_type":"code","source":["comp_cols = [f'comp_{c}' for c in ALL_COMP]\n","\n","exp_agg = exp.groupby('id').agg(\n","    total_past_exp   = ('is_past_exp',    'sum'),\n","    f500_past_exp    = ('is_f500',        'sum'),\n","    sum_years        = ('duration_years', 'sum'),\n","    mean_years       = ('duration_years', 'mean'),\n","    **{col: (col, 'max') for col in comp_cols}\n",").reset_index()\n","\n","exp_agg[comp_cols] = exp_agg[comp_cols].astype(int)\n","print(exp_agg.columns.tolist())"],"metadata":{"id":"YVrGHiSNRTYY","executionInfo":{"status":"ok","timestamp":1750437963209,"user_tz":-480,"elapsed":12,"user":{"displayName":"王廣瑜","userId":"13649321363543885870"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c0054281-aeac-4685-c2ef-e4fa2a04452c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['id', 'total_past_exp', 'f500_past_exp', 'sum_years', 'mean_years', 'comp_amazon', 'comp_apple', 'comp_google', 'comp_meta', 'comp_microsoft']\n"]}]},{"cell_type":"markdown","source":["#### proj"],"metadata":{"id":"6Xqlls9NaR2M"}},{"cell_type":"code","source":["# Create proj_text: combine title & description\n","proj['proj_text'] = (\n","    proj['proj_title'].fillna('') + ' ' +\n","    proj['proj_description'].fillna('')\n",").str.strip()\n","proj_combined = (\n","    proj.groupby('id')['proj_text']\n","        .apply(lambda texts: ' '.join([t for t in texts if t]))\n","        .reset_index()\n",")\n","proj_agg = (id_df.merge(proj_combined, on='id', how='left'))"],"metadata":{"id":"qBxcUS2SZLRs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### course"],"metadata":{"id":"-HpHh2DuccO1"}},{"cell_type":"code","source":["course = (\n","    df[['id','courses']]\n","      .explode('courses')\n","      .dropna(subset=['courses'])\n",")\n","course = pd.DataFrame({\n","    'id': course['id'],\n","    'course_title': course['courses'].map(lambda x: x.get('title') if isinstance(x, dict) else x)\n","})\n","course_agg = (\n","    course.groupby('id')['course_title']\n","          .agg(lambda titles: ' [SEP] '.join(titles))\n","          .reset_index()\n","          .rename(columns={'course_title': 'course_text'})\n",")\n","course_count = course.groupby('id').size().reset_index(name='n_course')"],"metadata":{"id":"bwP7uIwmgBKG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### cert"],"metadata":{"id":"FiGh398pgCLh"}},{"cell_type":"code","source":["cert = (\n","    df[['id','certifications']]\n","      .explode('certifications')\n","      .dropna(subset=['certifications'])\n",")\n","cert = pd.DataFrame({\n","    'id': cert['id'],\n","    'cert_title': cert['certifications'].map(lambda x: x.get('title') if isinstance(x, dict) else x)\n","})\n","cert_agg = (\n","    cert.groupby('id')['cert_title']\n","        .agg(lambda titles: ' [SEP] '.join(titles))\n","        .reset_index()\n","        .rename(columns={'cert_title': 'cert_text'})\n",")\n","cert_count = cert.groupby('id').size().reset_index(name='n_cert')"],"metadata":{"id":"iOsQ5MfIgDgr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### pub"],"metadata":{"id":"OQTcGVMPgKYi"}},{"cell_type":"code","source":["pub = (\n","    df[['id','publications']]\n","      .explode('publications')\n","      .dropna(subset=['publications'])\n",")\n","pub = pd.DataFrame({\n","    'id': pub['id'],\n","    'pub_title': pub['publications'].map(lambda x: x.get('title') if isinstance(x, dict) else x)\n","})\n","pub_agg = (\n","    pub.groupby('id')['pub_title']\n","       .agg(lambda titles: ' [SEP] '.join(titles))\n","       .reset_index()\n","       .rename(columns={'pub_title': 'pub_text'})\n",")\n","pub_count = pub.groupby('id').size().reset_index(name='n_pub')"],"metadata":{"id":"AHSBbsSVgFyg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Merge to df_final\n","df_final = (\n","    df\n","    .merge(cert_agg,   on='id', how='left')\n","    .merge(cert_count, on='id', how='left')\n","    .merge(course_agg, on='id', how='left')\n","    .merge(course_count,on='id', how='left')\n","    .merge(pub_agg,    on='id', how='left')\n","    .merge(pub_count,  on='id', how='left')\n","    .merge(edu_agg,    on='id', how='left')\n","    .merge(exp_agg,    on='id', how='left')\n","    .merge(proj_agg,   on='id', how='left')\n",")\n","\n","# Fiil 0 to empty spaces\n","df_final[['n_cert','n_course','n_pub']] = (\n","    df_final[['n_cert','n_course','n_pub']]\n","    .fillna(0)\n","    .astype(int)\n",")\n","\n","columns_to_drop = [\n","    'education', 'experience', 'projects', 'courses',\n","    'certifications', 'publications', 'name',\n","    'country_code', 'position', 'url',\n","    'current_company_company_id', 'current_company_name'\n","]\n","\n","df_final = df_final.drop(columns=columns_to_drop, errors='ignore')\n","df_final['label'] = LABELS[ROLE]"],"metadata":{"id":"_-w0mG7TczDX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_final.to_csv(OUTPUT, index=False)\n","print(OUTPUT + ' is saved.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vN8mNTQIhgwK","executionInfo":{"status":"ok","timestamp":1750437964010,"user_tz":-480,"elapsed":205,"user":{"displayName":"王廣瑜","userId":"13649321363543885870"}},"outputId":"d8c09e5c-2b6b-468c-8dc6-63c22e264f3f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cleaned/LGBM/cleaned_apple_swe.csv is saved.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"I5DLaf-hKkrG"},"execution_count":null,"outputs":[]}]}