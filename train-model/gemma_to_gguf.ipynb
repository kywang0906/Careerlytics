{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"F8lnBIGv1eSh"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","FOLDERNAME = \"Colab\\ Notebooks/fetch-data\"\n","%cd drive/MyDrive/$FOLDERNAME"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4auc0KLdBFHO"},"outputs":[],"source":["# !pip install -q transformers peft bitsandbytes accelerate llama-cpp-python"]},{"cell_type":"markdown","metadata":{"id":"FVWPdGYKJ8va"},"source":["## Merge"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":414,"status":"ok","timestamp":1751026783809,"user":{"displayName":"王廣瑜","userId":"13649321363543885870"},"user_tz":-480},"id":"WiISMksFBIQr","outputId":"e3bc5c8a-423b-497f-b939-aa0a4085192c"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["Token is valid (permission: fineGrained).\n","The token `llama-2` has been saved to /root/.cache/huggingface/stored_tokens\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful.\n","Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"]}],"source":["%%bash\n","export HF_TOKEN=\"MY_TOKEN\"\n","huggingface-cli login --token $HF_TOKEN --add-to-git-credential"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wnat-hm2AEWo"},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from peft import PeftModel\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\", use_fast=True)\n","\n","# Load base model to GPU\n","base = AutoModelForCausalLM.from_pretrained(\n","    \"google/gemma-2b\",\n","    device_map=\"auto\",            # Automatically put model onto GPU\n","    torch_dtype=torch.float16,\n","    offload_folder=None,\n","    offload_state_dict=False\n",")\n","\n","peft_model = PeftModel.from_pretrained(base, \"lora_gemma2_resume\")\n","peft_model.eval()\n","\n","# Merge LoRA\n","merged = peft_model.merge_and_unload()\n","\n","merged.save_pretrained(\"gemma2_merged\")"]},{"cell_type":"markdown","metadata":{"id":"IMjm-QmKJ_Zc"},"source":["## GGUF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6_5YefrKIiY_"},"outputs":[],"source":["!apt-get update && apt-get install -y cmake"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SO40Q6TkJuyC"},"outputs":[],"source":["!git clone https://github.com/ggml-org/llama.cpp.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pWmNFzLfIxeu"},"outputs":[],"source":["%cd llama.cpp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gRYYAPGSIykh"},"outputs":[],"source":["!cmake -B build -S ."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MsxmgBVzKuul"},"outputs":[],"source":["!cmake --build build -- -j$(nproc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gxxDS0GtNpN7"},"outputs":[],"source":["%cd .."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lh-B3i0gOrdK"},"outputs":[],"source":["!pip install -q huggingface_hub"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"49U8vdHj1Jca"},"outputs":[],"source":["from huggingface_hub import hf_hub_download\n","import shutil, os\n","\n","os.makedirs(\"gemma2_merged\", exist_ok=True)\n","\n","for fn in [\"tokenizer.model\", \"tokenizer.json\"]:\n","    try:\n","        src = hf_hub_download(\n","            repo_id=\"google/gemma-2b\",\n","            filename=fn,\n","            repo_type=\"model\",\n","            use_auth_token=True\n","        )\n","        shutil.copy(src, \"gemma2_merged/\")\n","        print(f\"✔ copied {fn}\")\n","    except Exception as e:\n","        print(f\"✘ {fn} not found or error: {e}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MCI42RMYO_Wr"},"outputs":[],"source":["!cp gemma2_original/tokenizer.model      gemma2_merged/\n","!cp gemma2_original/tokenizer.json       gemma2_merged/\n","!cp gemma2_original/vocab.json           gemma2_merged/ 2>/dev/null || true\n","!cp gemma2_original/merges.txt           gemma2_merged/ 2>/dev/null || true"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EoiZfxzWLfGy"},"outputs":[],"source":["!python3 llama.cpp/convert_hf_to_gguf.py \\\n","  gemma2_merged \\\n","  --outfile gemma2_merged-q8_0.gguf \\\n","  --outtype q8_0"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNp3pccUbpIhdjcOZ5TJOgJ","gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
